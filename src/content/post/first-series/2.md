---
title: How Configure K8s cluster with Cilium
slug: how-configure-k8s-cluster-with-cilium
tags:
  - 블로그
published: 2025-08-02
draft: false
---

# eBPF

cilium 은 eBPF를 사용해서 Pod 간의 빠르고 효율적인 네트워킹를 지원한다.
여기서 eBPF는 리눅스 커널에 샌드박스를 생성하고 사용자 코드를 삽입할 수 있는 기술을 말한다.
cilium은 eBPF 기술을 기반으로 리눅스 커널에 네트워킹 로직을 주입하여 클러스터 간 통신 성능을 개선시킨 듯 하다.

# Cilium을 k8s 클러스터에 설치하기

우선 cilium 을 설치하는 방법은 helm을 써도 되고, git 저장소를 통해 cilium cli 를 설치해도 된다.

helm 으로 설치하던, cilium cli로 설치하던 둘 다 사용할 필요성이 있어보인다.
cilium을 설치한 이후에 클러스터 내 노드 간 통신 테스트를 할 때도 그렇고 (cilium connectivity test), helm 차트로 cilium 설정을 업데이트한 후에 연결 상태를 확인해볼 때(cilium status --wait)도 그렇고, 나는 cilium cli를 많이 사용하고 있다.

~~공식 문서 상에서 cilium 은 kubernates native ingress 를 지원한다. cilium ingress를 활성화하면 클러스터 외부에서 들어오는 트래픽에 대한 제어가 가능해진다.
이를 사용하기 위한 prerequired 목록은 cilium 의 설정을 업데이트하는 것부터 시작한다.

cilium은 kubernetes gateway api와 통합하여 L7 계층에서 트래픽 라우팅을 수행할 수 있다. 하지만 아직 Beta 버전으로 클러스터 내 트래픽 라우팅 시 필요한 기능들(외부 인증 서비스 프록시, 헤더 커스텀)에 제한된 사항이 많으므로 규모 있는 클러스터 구성에 어려움이 있다.

# Cilium 설정

kubeProxyReplacement=true 로 설정하여 kubernetes의 프록시 기능을 cilium의 네트워크 스택이 담당하도록 바꾸는 설정으로 보인다.

~~ingressController.enabled=true : 해당 설정이 cilium의 ingress 지원을 위한 활성화 옵션이다.
ingressController.loadbalancerMode={"shared"|"dedicated"} : cilium ingress의 모드 설정 옵션이다. (shared, dedicated가 무슨 차이일까? : 이에 대한 답변: cilium ingress controller가 ingress 리소스를 클러스터에 적용하면 자동으로 LoadBalance 타입의 Kubernetes Service가 실행된다. 여기서 shared 모드일 경우 단일 LoadBalance 서비스가 생성되고 이 서비스가 모든 Ingress를 위해 재사용된다. 반대로 dedicated 모드일 경우 각 Ingress마다 개별 LoadBalance 서비스가 생성된다. )

l2announcements.enabled: true : 정의된 L2Announcement에 의해 Cilium이 클러스터 내 External IP를 광고

위 설정을 적용하고나면 (helm upgrade cilium cilium/cilium -n kube-system ---reuse-values -f <values.yaml 경로>) cilium을 클러스터의 cni로 활용하기 위한 사전 준비를 마치게된다.

# IP Pool, Network Policy, L2Announcement

내 클러스터는 두 대의 장비로 구성하고 있고, 이 두 장비는 한 대의 일반 가정용 라우터에 유선으로 연결되어있다. 클라우드 서비스를 이용한다면 클라우드 공급자에 의해 LoadBalancer의 IP를 할당받을 수 있으나, 내 상황은 프라이빗 클라우드 환경이므로 자체적인 LoadBalancer 를 구축하고 따로 IP를 할당할 방법이 필요하다.

## IP Pool

cilium 은 LoadBalancer IP Address Management, LB IPAM 기능을 제공한다. 이는 특정 서비스 타입에 대해 Cilium이 External IP를 할당할 수 있는 기능이다. 이 기능은 휴먼 상태로 활성화되어있으며 클러스터 내 IP Pool 자원이 추가되면 동작한다.

따라서 앞으로 생성될 LoadBalancer에 External IP를 부여하기 위해 LB IPAM을 사용하기 위해서는 먼저 클러스터가 사용할 수 있는 External IP 영역을 확보해야한다. 이를 위해서는 CiliumLoadBalancerIPPool 자원을 생성해야한다.
`spec.blocks.cidr` 를 직접 지정하거나 `spec.blocks.start` 및 `spec.blocks.stop` 을 명시해서 범위 안의 IP 영역을 확보한다.

이 후 `kubectl get ippools` 명령으로 IPPool 자원을 확인하고 할당 가능한 IP 개수를 확인할 수 있다.

( Image here )

여기까지 진행한다면, IP Pool 자원이 확보한 IP 영역을 LB IPAM이 관리할 수 있다. 하지만 LB IPAM에 의해 할당된 External IP로 외부 트래픽이 접근하기 위해서는 해당 IP로 들어오는 ARP 요청에 대해 응답할 수 있어야한다.

## L2 Announcement

서비스(해당 글에서 LoadBalancer)의 External IP로 들어오는 ARP 요청에 대해 응답하는 방법 중 하나로, Cilium 의 L2 Announcement / L2 Aware LB 기능을 사용할 수 있다. 여기서 ExternalIP를 할당할 서비스 타입, 연결할 네트워크 인터페이스를 정의할 수 있다.

## Envoy Gateway

전통적인 Ingress 방식의 한계점을 극복하기 위해 K8S 팀은 Application Layer 에서 접근 제어 및 필터링 등 트래픽 제어를 위한 자원을 명시적으로 분리한 Gateway API를 제안했다. Envoy Gateway는 K8S Gateway API와 통합되어 기존 Envoy Proxy의 기능을 Gateway로 확장시켰다.

1. Envoy Gateway Class 자원 생성
2. Envoy Proxy 자원 생성 후 Gateway class에 연결
3. Envoy Gateway 생성
4. HTTPRoute, TLSRoute 생성 후 서비스 연결 및 Gateway에 연결
